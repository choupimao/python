# 案例：使用正则表达式的爬虫

现在拥有了正则表达式这把神兵利器，我们就可以进行对爬取到的全部网页源代码进行筛选了。

下面我们一起尝试一下爬取内涵段子网站： <http://www.neihan8.com/article/list_5_1.html>

打开之后，不难看到里面一个一个灰常有内涵的段子，当你进行翻页的时候，注意url地址的变化：

- 第一页url: http: //www.neihan8.com/article/list_5_1 .html
- 第二页url: http: //www.neihan8.com/article/list_5_2 .html
- 第三页url: http: //www.neihan8.com/article/list_5_3 .html
- 第四页url: http: //www.neihan8.com/article/list_5_4 .html

这样我们的url规律找到了，要想爬取所有的段子，只需要修改一个参数即可。 下面我们就开始一步一步将所有的段子爬取下来吧。

## 第一步：获取数据

### 1.**按照我们之前的用法，我们需要写一个加载网页的方法。**

这里我们统一定义一个类，将url请求作为一个成员方法处理。

我们创建一个文件，叫duanzi_spider.py

然后定义一个Spider类，并且添加一个加载页面的成员方法

```python
class Spider:
    """
    内涵段子爬虫类
    """
    def load_page(self,page):
        """
        @brief 定义一个url请求网页的方法
        @param page 需要请求的第几页
        @returns 返回的页面html
        :param page:
        :return:
        """
        url = "http://www.neihanpa.com/article/list_5_" + str(page) +".html"
        user_agent = "Baiduspider"
        headers = {"User-Agent":user_agent}
        request = urllib2.Request(url)
        response = urllib2.urlopen(request)
        html = response.read()
        print(html)

    #return html
```

**以上的loadPage的实现体想必大家应该很熟悉了，需要注意定义Python类的成员方法需要额外添加一个参数self**

- 那么loadPage(self,page)中的page是我们指定去请求第几页
- 然后通过print(html)打印到屏幕上
- 然后我们写一个main函数见到测试一个loadPage方法

### 2.**写main函数测试一个loadPage方法**

```python
if __name__ == '__main__':
    """
    ==================
    内涵段子小爬虫
    ==================
    """
    print('请按下回车开始')
    raw_input()

    #定义一个Spider对象
    mySpider = Spider()
    mySpider.load_page(1)
```

程序正常执行的话，我们会在屏幕上打印了内涵段子第一页的全部html代码。 但是我们发现，html中的中文部分显示的可能是乱码 。

**那么我们需要简单的将得到的网页源代码处理一下：**

```python
        html = response.read()
        gbk_html = html.decode('gb2312').encode('utf-8')
        # print(html)
        # print(gbk_html)
        return gbk_html
```

​	注意：对于每个网站对中文编码都不同，html.decode('gbk')不是同用的

## 第二步：筛选数据

接下来我们已经得到了整个页面的数据，但是，很多内容我们并不关心，所以下一步我们需要进行筛选，如何筛选就用到了上一节讲述的正则表达式

- 首先

```wiki
import re
```

- 然后，在我们得到的gbk_html中进行筛选匹配。

### 我们需要一个匹配规则：

​	我们可以打开内涵段子的网页，鼠标点击右键“查看源代码”，我们需要的段子内容都是在一个<div>标签中，而且每个div 都有一个属性class="f18 mb20"

**所以，我们只需要匹配到网页中所有<div class="f18 mb20">到</div>的数据就可以了**

根据正则表达式，我们可以推算出一个公式是：

<div.*?class="f18 mb20">(.*?)</div>

- 这个表达式实际上就是匹配到所以的div中class="f18 mb20"里面的内容
- 然后将这个正则应用到代码中，我们会得到以下代码；

```python

        #找到所有段子内容<div class = 'f18 mb20'><div>
        #re.S如果没有re.S则只是匹配一行没有符合规则的字符串，没有则下一行重新匹配
        #如果加上re.S则是将所以的字符串将一个整体进行匹配
        pattern = re.compile(r'<div.*?class = 'f18 mb20'>(.*?)<div>',re.S)
        item_list = pattern.findall(gbk_html)

        return item_list

    def print_one_page(self,item_list,page):
        """
        @brief 处理得到的段子列表
        :param item_list得到的段子列表
        :param page: 处理第几页
        :return:
        """
        print("**********第%d页 爬取完毕...******"%page)
        for item in item_list:
            print("="*18)
            print(item)
```

- 这里需要注意一个是re.S是正则表达式中匹配的一个参数
- 如果没有re.S则是只匹配一行 有没有符合规则的字符串，如果没有则下一行重新匹配
- 如果加上re.S则是将所以的字符串将一个整体进行匹配，findall将所以匹配的结果封装到一个list中

然后我们写了一个遍历item_list的一个方法print_one_page()，执行一下



我们第一页的全部段子，不包含其他信息全部的打印了出来。

- 你会发现段子中有很多<p>,<p> 很是不舒服，实际上这个是html一种段落的标签。
- 在浏览器上看不出来，但是如果按照文本打印会有<p>出现，那么我们只需要把我们不希望的内容去掉即可
- 我们可以如下简单修改一个print_one_page()

```python
        for item in item_list:
            print("="*18)
            item = item.replace("<p>", "").replace("</p>", "").replace("<br />", "")
            print(item)
```



## 第三步：保存数据

- 我们可以将所以的段子存放在文件中。比如，我们可以将得到的每个item不是打印出来，而是存放在一个叫duanzi.txt的文件中也可以

```python
    def write_to_file(self,text):
        """
        将数据追加写入文件中
        :param text:文件内容
        :return:
        """
        my_file = open('./duanzi.txt','a') #追加形式打开文件
        my_file.write(text)
        my_file.write("-"*30)
        my_file.close()
```

- 然后将我们的print的语句改为write_to_file()，当前页面的所以段子就存在了本地的dunazi.txt文件中

```python
def print_one_page(self, item_list, page):

        # print item
        self.writeToFile(item)
```

## 第四步：显示数据

- 接下来我们就通过参数的传递对page进行叠加来遍历 内涵段子吧的全部段子内容
- 只需要在外层加一些逻辑处理即可

```python
def doWork(self):
'''
    让爬虫开始工作
'''
    while self.enable:
        try:
            item_list = self.loadPage(self.page)
        except urllib2.URLError, e:
            print e.reason
            continue

        #对得到的段子item_list处理
        self.printOnePage(item_list, self.page)
        self.page += 1 #此页处理完毕，处理下一页
        print "按回车继续..."
        print "输入 quit 退出"
        command = raw_input()
        if (command == "quit"):
            self.enable = False
            break
```

> - 最后，我们执行我们的代码，完成后查看当前路径下的duanzi.txt文件，里面已经有了我们要的内涵段子。

**以上便是一个非常精简使用的小爬虫程序，使用起来很是方便，如果想要爬取其他网站的信息，只需要修改其中某些参数和一些细节就行了。**





